1. NLP & Dataset Prep – Select a public text dataset Amazon. Briefly explain why you chose it and the preprocessing steps before training (cleaning, tokenization, etc.).

2. Prompt Engineering & Model Interaction – Using a pretrained LLM (OpenAI GPT, Hugging Face, etc.), craft 3 distinct prompt variations for a specific task (e.g., summarization, sentiment analysis, Q&A). Explain your rationale and how outputs may differ.

3. Model Fine-Tuning/Evaluation (optional if limited by compute) – Fine-tune a lightweight model or evaluate a pretrained model on your prompts. Report metrics (accuracy, precision, recall, F1) or qualitative output analysis. Aim for 90%+ accuracy where possible.

4. Troubleshooting – Describe one likely issue in prompt tuning or model training (e.g., overfitting, bias) and propose solutions.
